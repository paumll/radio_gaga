{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import glob\n",
    "from random import seed, shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.notes_to_index = {}\n",
    "        self.index_to_notes = {}\n",
    "        self.num_of_word = 0\n",
    "        self.unique_word = 0\n",
    "        self.notes_freq = {}\n",
    "        \n",
    "    def transform(self,list_array):\n",
    "        \"\"\" Transform a list of note in string into index.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        list_array : list\n",
    "            list of note in string format\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        The transformed list in numpy array.\n",
    "        \n",
    "        \"\"\"\n",
    "        transformed_list = []\n",
    "        for instance in list_array:\n",
    "            transformed_list.append([self.notes_to_index[note] for note in instance])\n",
    "        return np.array(transformed_list, dtype=np.int32)\n",
    " \n",
    "    def partial_fit(self, notes):\n",
    "        \"\"\" Partial fit on the dictionary of the tokenizer\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        notes : list of notes\n",
    "        \n",
    "        \"\"\"\n",
    "        for note in notes:\n",
    "            note_str = ','.join(str(a) for a in note)\n",
    "            if note_str in self.notes_freq:\n",
    "                self.notes_freq[note_str] += 1\n",
    "                self.num_of_word += 1\n",
    "            else:\n",
    "                self.notes_freq[note_str] = 1\n",
    "                self.unique_word += 1\n",
    "                self.num_of_word += 1\n",
    "                self.notes_to_index[note_str], self.index_to_notes[self.unique_word] = self.unique_word, note_str\n",
    "            \n",
    "    def add_new_note(self, note):\n",
    "        \"\"\" Add a new note into the dictionary\n",
    "        Parameters\n",
    "        ==========\n",
    "        note : str\n",
    "          a new note who is not in dictionary.  \n",
    "        \"\"\"\n",
    "        assert note not in self.notes_to_index\n",
    "        self.unique_word += 1\n",
    "        self.notes_to_index[note], self.index_to_notes[self.unique_word] = self.unique_word, note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resta de funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_song(list_all_midi, batch_music=16, start_index=0, fs=30, seq_len=50, use_tqdm=False):\n",
    "    \"\"\"\n",
    "    Generate Batch music that will be used to be input and output of the neural network\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    list_all_midi : list\n",
    "      List of midi files\n",
    "    batch_music : int\n",
    "      A number of music in one batch\n",
    "    start_index : int\n",
    "      The start index to be batched in list_all_midi\n",
    "    fs : int\n",
    "      Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    seq_len : int\n",
    "      The sequence length of the music to be input of neural network\n",
    "    use_tqdm : bool\n",
    "      Whether to use tqdm or not in the function\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of input and target neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(list_all_midi) >= batch_music\n",
    "    dict_time_notes = generate_dict_time_notes(list_all_midi, batch_music, start_index, fs, use_tqdm=use_tqdm)\n",
    "    \n",
    "    list_musics = process_notes_in_song(dict_time_notes, seq_len)\n",
    "    collected_list_input, collected_list_target = [], []\n",
    "     \n",
    "    for music in list_musics:\n",
    "        list_training, list_target = generate_input_and_target(music, seq_len)\n",
    "        collected_list_input += list_training\n",
    "        collected_list_target += list_target\n",
    "    return collected_list_input, collected_list_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict_time_notes(list_all_midi, batch_song = 16, start_index=0, fs=30, use_tqdm=True):\n",
    "    \"\"\" Generate map (dictionary) of music ( in index ) to piano_roll (in np.array)\n",
    "    Parameters\n",
    "    ==========\n",
    "    list_all_midi : list\n",
    "        List of midi files\n",
    "    batch_music : int\n",
    "      A number of music in one batch\n",
    "    start_index : int\n",
    "      The start index to be batched in list_all_midi\n",
    "    fs : int\n",
    "      Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    use_tqdm : bool\n",
    "      Whether to use tqdm or not in the function\n",
    "    Returns\n",
    "    =======\n",
    "    dictionary of music to piano_roll (in np.array)\n",
    "    \"\"\"\n",
    "    assert len(list_all_midi) >= batch_song\n",
    "    \n",
    "    dict_time_notes = {}\n",
    "    process_tqdm_midi = tqdm(range(start_index, min(start_index + batch_song, len(list_all_midi)))) if use_tqdm else range(start_index,  min(start_index + batch_song, len(list_all_midi)))\n",
    "    for i in process_tqdm_midi:\n",
    "        midi_file_name = list_all_midi[i]\n",
    "        if use_tqdm:\n",
    "            process_tqdm_midi.set_description(\"Processing {}\".format(midi_file_name))\n",
    "        try: # Handle exception on malformat MIDI files\n",
    "            midi_pretty_format = pretty_midi.PrettyMIDI(midi_file_name)\n",
    "            piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
    "            piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
    "            dict_time_notes[i] = piano_roll\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"broken file : {}\".format(midi_file_name))\n",
    "            pass\n",
    "    return dict_time_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_and_target(dict_keys_time, seq_len=50):\n",
    "    \"\"\" Generate input and the target of our deep learning for one music.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_keys_time : dict\n",
    "      Dictionary of timestep and notes\n",
    "    seq_len : int\n",
    "      The length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of list of input and list of target of neural network.\n",
    "    \n",
    "       \n",
    "    \"\"\"\n",
    "    # Get the start time and end time\n",
    "    start_time, end_time = list(dict_keys_time.keys())[0], list(dict_keys_time.keys())[-1]\n",
    "    list_training, list_target = [], []\n",
    "    for index_enum, time in enumerate(range(start_time, end_time)):\n",
    "        list_append_training, list_append_target = [], []\n",
    "        start_iterate = 0\n",
    "        flag_target_append = False # flag to append the test list\n",
    "        if index_enum < seq_len:\n",
    "            start_iterate = seq_len - index_enum - 1\n",
    "            for i in range(start_iterate): # add 'e' to the seq list. \n",
    "                list_append_training.append('e')\n",
    "                flag_target_append = True\n",
    "\n",
    "        for i in range(start_iterate,seq_len):\n",
    "            index_enum = time - (seq_len - i - 1)\n",
    "            if index_enum in dict_keys_time:\n",
    "                list_append_training.append(','.join(str(x) for x in dict_keys_time[index_enum]))      \n",
    "            else:\n",
    "                list_append_training.append('e')\n",
    "\n",
    "        # add time + 1 to the list_append_target\n",
    "        if time+1 in dict_keys_time:\n",
    "            list_append_target.append(','.join(str(x) for x in dict_keys_time[time+1]))\n",
    "        else:\n",
    "            list_append_target.append('e')\n",
    "        list_training.append(list_append_training)\n",
    "        list_target.append(list_append_target)\n",
    "    return list_training, list_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notes_in_song(dict_time_notes, seq_len = 50):\n",
    "    \"\"\"\n",
    "    Iterate the dict of piano rolls into dictionary of timesteps and note played\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_time_notes : dict\n",
    "      dict contains index of music ( in index ) to piano_roll (in np.array)\n",
    "    seq_len : int\n",
    "      Length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Dict of timesteps and note played\n",
    "    \"\"\"\n",
    "    list_of_dict_keys_time = []\n",
    "    \n",
    "    for key in dict_time_notes:\n",
    "        sample = dict_time_notes[key]\n",
    "        times = np.unique(np.where(sample > 0)[1])\n",
    "        index = np.where(sample > 0)\n",
    "        dict_keys_time = {}\n",
    "\n",
    "        for time in times:\n",
    "            index_where = np.where(index[1] == time)\n",
    "            notes = index[0][index_where]\n",
    "            dict_keys_time[time] = notes\n",
    "        list_of_dict_keys_time.append(dict_keys_time)\n",
    "    return list_of_dict_keys_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_midi(folder = '../maestro-v2.0.0/**/*.midi', seed_int = 666):\n",
    "    \"\"\"Get the list of all midi file in the folders\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    folder : str\n",
    "      The midi folder.\n",
    "    seed_int : int\n",
    "      the random seed.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    The midi files\n",
    "    \n",
    "    \"\"\"\n",
    "    list_all_midi = glob.glob(folder)\n",
    "    seed(seed_int)\n",
    "    shuffle(list_all_midi)\n",
    "    return list_all_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/maestro-v2.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:33<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "fs = 15                                                                          #PARM\n",
    "window_size = 50                                                                #PARM\n",
    "\n",
    "\n",
    "\n",
    "list_all_midi = get_list_midi(folder = DATA_PATH + '**/*.midi')\n",
    "\n",
    "midi_list = list_all_midi[:100]\n",
    "\n",
    "batch = 1\n",
    "start_index = 0\n",
    "note_tokenizer = NoteTokenizer()\n",
    "\n",
    "for i in tqdm(range(len(midi_list))):\n",
    "    dict_time_notes = generate_dict_time_notes(midi_list, batch_song=1, start_index=i, use_tqdm=False, fs=fs)\n",
    "    full_notes = process_notes_in_song(dict_time_notes)\n",
    "    for note in full_notes:\n",
    "        note_tokenizer.partial_fit(list(note.values()))\n",
    "\n",
    "note_tokenizer.add_new_note('e') # Add empty notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MUSIC:  14%|█▍        | 1/7 [05:25<32:30, 325.05s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-81f3dcb71c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     inputs_nnet_large, outputs_nnet_large = generate_batch_song(\n\u001b[1;32m      5\u001b[0m         \u001b[0mmidi_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         seq_len=50, use_tqdm=False) # We use the function that have been defined here\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minputs_nnet_large\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_nnet_large\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutputs_nnet_large\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_nnet_large\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6c6dbea143fd>\u001b[0m in \u001b[0;36mgenerate_batch_song\u001b[0;34m(list_all_midi, batch_music, start_index, fs, seq_len, use_tqdm)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_all_midi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_music\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdict_time_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dict_time_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_all_midi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_music\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlist_musics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_notes_in_song\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_time_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-56e23da38262>\u001b[0m in \u001b[0;36mgenerate_dict_time_notes\u001b[0;34m(list_all_midi, batch_song, start_index, fs, use_tqdm)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprocess_tqdm_midi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Handle exception on malformat MIDI files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmidi_pretty_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretty_midi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyMIDI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mpiano_midi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_pretty_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get the piano channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mpiano_roll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpiano_midi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_piano_roll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pretty_midi/pretty_midi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, midi_file, resolution, initial_tempo)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Populate the list of instruments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_instruments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pretty_midi/pretty_midi.py\u001b[0m in \u001b[0;36m_load_instruments\u001b[0;34m(self, midi_data)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'control_change'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                     control_change = ControlChange(\n\u001b[0;32m--> 375\u001b[0;31m                         \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m                         self.__tick_to_time[event.time])\n\u001b[1;32m    377\u001b[0m                     \u001b[0;31m# Get the program for the current inst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps=0\n",
    "for i in tqdm(range(0,100, 16), desc='MUSIC'):\n",
    "    steps += 1\n",
    "    inputs_nnet_large, outputs_nnet_large = generate_batch_song(\n",
    "        midi_list, 100, start_index=i, fs=fs, \n",
    "        seq_len=50, use_tqdm=False) # We use the function that have been defined here\n",
    "    inputs_nnet_large = np.array(note_tokenizer.transform(inputs_nnet_large), dtype=np.int32)\n",
    "    outputs_nnet_large = np.array(note_tokenizer.transform(outputs_nnet_large), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs_nnet_large' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7790058d034b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs_nnet_large\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs_nnet_large' is not defined"
     ]
    }
   ],
   "source": [
    "inputs_nnet_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, token_size):\n",
    "    \"\"\"Yield elements from data in chunks with a maximum of batch_size sequences and token_size tokens.\"\"\"\n",
    "    minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "    for ex in data:\n",
    "        seq_len = len(ex[0])\n",
    "        if seq_len > token_size:\n",
    "            ex = (ex[0][:token_size], ex[1])\n",
    "            seq_len = token_size\n",
    "        minibatch.append(ex)\n",
    "        sequences_so_far += 1\n",
    "        tokens_so_far += seq_len\n",
    "        if sequences_so_far == batch_size or tokens_so_far == token_size:\n",
    "            yield minibatch\n",
    "            minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n",
    "        elif sequences_so_far > batch_size or tokens_so_far > token_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, sequences_so_far, tokens_so_far = minibatch[-1:], 1, len(minibatch[-1][0])\n",
    "    if minibatch:\n",
    "        yield minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_generator(data, batch_size, token_size, shuffle=False):\n",
    "    \"\"\"Sort within buckets, then batch, then shuffle batches.\n",
    "    Partitions data into chunks of size 100*token_size, sorts examples within\n",
    "    each chunk, then batch these examples and shuffle the batches.\n",
    "    \"\"\"\n",
    "    for p in batch_generator(data, batch_size * 100, token_size * 100):\n",
    "        p_batch = batch_generator(sorted(p, key=lambda t: len(t[0]), reverse=True), batch_size, token_size)\n",
    "        p_list = list(p_batch)\n",
    "        if shuffle:\n",
    "            for b in random.sample(p_list, len(p_list)):\n",
    "                yield b\n",
    "        else:\n",
    "            for b in p_list:\n",
    "                yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, model=\"lstm\", num_layers=1, bidirectional=False, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.model = model.lower()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = torch.nn.Embedding(input_size, embedding_size, padding_idx=pad_idx)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = torch.nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = torch.nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, input_lengths):\n",
    "        # T x B\n",
    "        encoded = self.embed(input)\n",
    "        # T x B x E\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(encoded, input_lengths)\n",
    "        # Packed T x B x E\n",
    "        output, _ = self.rnn(packed)\n",
    "        # Packed T x B x H\n",
    "        # Important: you may need to replace '-inf' with the default zero padding for other pooling layers\n",
    "        padded, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float('-inf'))\n",
    "        # T x B x H\n",
    "        output, _ = padded.max(dim=0)\n",
    "        # B x H\n",
    "        output = self.h2o(output)\n",
    "        # B x O\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CUDA is not available. Select 'GPU On' on kernel settings\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA is not available. Select 'GPU On' on kernel settings\")\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.manual_seed(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, batch_size, token_size, log=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    ntokens = 0\n",
    "    niterations = 0\n",
    "    for batch in pool_generator(data, batch_size, token_size, shuffle=True):\n",
    "        # Get input and target sequences from batch\n",
    "        X = [torch.from_numpy(d[0]) for d in batch]\n",
    "        X_lengths = [x.numel() for x in X]\n",
    "        ntokens += sum(X_lengths)\n",
    "        X_lengths = torch.tensor(X_lengths, dtype=torch.long, device=device)\n",
    "        y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "        # Pad the input sequences to create a matrix\n",
    "        X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(X, X_lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Training statistics\n",
    "        total_loss += loss.item()\n",
    "        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n",
    "        nsentences += y.numel()\n",
    "        niterations += 1\n",
    "    \n",
    "    total_loss = total_loss / nsentences\n",
    "    accuracy = 100 * ncorrect / nsentences\n",
    "    if log:\n",
    "        print(f'Train: wpb={ntokens//niterations}, bsz={nsentences//niterations}, num_updates={niterations}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data, batch_size, token_size):\n",
    "    model.eval()\n",
    "    # calculate accuracy on validation set\n",
    "    ncorrect = 0\n",
    "    nsentences = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pool_generator(data, batch_size, token_size):\n",
    "            # Get input and target sequences from batch\n",
    "            X = [torch.from_numpy(d[0]) for d in batch]\n",
    "            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long, device=device)\n",
    "            y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n",
    "            # Pad the input sequences to create a matrix\n",
    "            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n",
    "            answer = model(X, X_lengths)\n",
    "            ncorrect += (torch.max(answer, 1)[1] == y).sum().item()\n",
    "            nsentences += y.numel()\n",
    "        dev_acc = 100 * ncorrect / nsentences\n",
    "    return dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-911334159a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbidirectional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "embedding_size = 64\n",
    "bidirectional = False\n",
    "ntokens = len(char_vocab)\n",
    "nlabels = len(lang_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional, pad_idx=pad_index).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, token_size = 256, 200000\n",
    "epochs = 25\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "model, optimizer = get_model()\n",
    "print(f'Training cross-validation model for {epochs} epochs')\n",
    "t0 = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    acc = train(model, optimizer, train_data, batch_size, token_size, log=epoch==1)\n",
    "    train_accuracy.append(acc)\n",
    "    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}% ({time.time() - t0:.0f}s)')\n",
    "    acc = validate(model, val_data, batch_size, token_size)\n",
    "    valid_accuracy.append(acc)\n",
    "    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
