{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.autograd import Variable\n","import os\n","import pickle\n","from tqdm import notebook, tqdm"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["with open('data/train_128.pickle', 'rb') as train_pickle:\n","    train_data = pickle.load(train_pickle)\n","    \n","with open('data/valid_128.pickle', 'rb') as valid_pickle:\n","    valid_data = pickle.load(valid_pickle)\n","\n","with open('data/test_128.pickle', 'rb') as test_pickle:\n","    test_data = pickle.load(test_pickle)"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class MultiResBlock(nn.Module):\n","    def __init__(self,ch_in,ch_out):\n","        super().__init__()\n","        self.conv1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0)\n","        self.bnorm1x1 = nn.BatchNorm2d(ch_out,track_running_stats=False)\n","        self.fconv = nn.Conv2d(ch_in, ch_out//6, kernel_size=3, padding=1)\n","        self.fbnorm = nn.BatchNorm2d(ch_out//6,track_running_stats=False)\n","        self.sconv = nn.Conv2d(ch_out//6, ch_out//3, kernel_size=3, padding=1)\n","        self.sbnorm = nn.BatchNorm2d(ch_out//3,track_running_stats=False)\n","        self.tconv = nn.Conv2d(ch_out//3, ch_out//2+1, kernel_size=3, padding=1)\n","        self.tbnorm = nn.BatchNorm2d(ch_out//2+1,track_running_stats=False)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self,x):\n","        res1x1 = self.relu(self.bnorm1x1(self.conv1x1(x)))\n","        #print(\"res1x1 done\")\n","        first = self.relu(self.fbnorm(self.fconv(x)))\n","        #print(\"fconv done\")\n","        second = self.relu(self.sbnorm(self.sconv(first)))\n","        third = self.relu(self.tbnorm(self.tconv(second)))\n","        resconv = torch.cat((first,second,third),dim=1)\n","        y = res1x1+resconv\n","        return y"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Generator_Res(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.tr1 = MultiResBlock(512, 256) \n","        self.tr2 = MultiResBlock(256, 128)\n","        self.tr3 = MultiResBlock(128, 64)\n","        self.tr4 = MultiResBlock(64, 32)\n","        self.tr5 = MultiResBlock(32, 16)\n","        self.tr6 = MultiResBlock(16, 8)\n","        self.tr7 = nn.Conv2d(8, 1, kernel_size = 3, stride = 1, padding = 1)\n","        self.relu = nn.ReLU()\n","        self.fc = nn.Linear(1024, 512*4*4)\n","        self.sigmoid = nn.Sigmoid()\n","        self.upsample = nn.Upsample(scale_factor = 2)\n","\n","    def forward(self, z):\n","        z = self.fc(z)\n","        z = z.view(-1, 512, 4, 4)\n","        z = self.tr1(z)\n","        z = self.upsample(z) # 8x8\n","        z = self.tr2(z)\n","        z = self.upsample(z) # 16x16\n","        z = self.tr3(z)\n","        z = self.upsample(z) # 32x32\n","        z = self.tr4(z)\n","        z = self.upsample(z) # 64x64\n","        z = self.tr5(z)\n","        z = self.upsample(z) # 128x128\n","        z = self.tr6(z)\n","        z = self.tr7(z)\n","        z = self.sigmoid(z)\n","        return z\n"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Generator_Res2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.tr1 = MultiResBlock(256, 128)\n","        self.tr2 = MultiResBlock(128, 64)\n","        self.tr3 = MultiResBlock(64, 32)\n","        self.tr4 = MultiResBlock(32, 16)\n","        self.tr5 = MultiResBlock(16, 8)\n","        self.tr6 = nn.Conv2d(8, 1, kernel_size = 3, stride = 1, padding = 1)\n","        self.relu = nn.ReLU()\n","        self.fc = nn.Linear(1024, 256*16*16)\n","        self.sigmoid = nn.Sigmoid()\n","        self.upsample = nn.Upsample(scale_factor = 2)\n","\n","    def forward(self, z):\n","        z = self.fc(z) \n","        z1 = z.view(-1, 256, 16, 16)\n","        z1 = self.tr1(z1) # z1 128x16x16\n","        z2 = self.upsample(z1)\n","        z2 = self.tr2(z2) # z2 64x32x32\n","        z3 = self.upsample(z2)\n","        z3 = self.tr3(z3) # z3 32x64x64\n","        z4 = self.upsample(z3)\n","        z4 = self.tr4(z4) # z4 16x128x128\n","        z5 = self.tr5(z4) # z5 8x128x128\n","        z6 = self.tr6(z5) # z6 1x128x128\n","        z_out = self.sigmoid(z6)\n","        return z_out, z1, z2, z3, z4"],"execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Discriminator_con(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size = 3, padding = 1)#, padding = 1)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size = 3, padding = 1)#, padding = 1)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n","        self.conv4 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n","        self.conv5 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n","        self.fc = nn.Linear(128*8*8, 1)\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.sigmoid = nn.Sigmoid()\n","        self.relu = nn.ReLU()\n","        \n","        self.cv1x1_1 = nn.Conv2d(1, 16, kernel_size = 1)\n","        self.cv1x1_2 = nn.Conv2d(1, 32, kernel_size = 1)\n","        self.cv1x1_3 = nn.Conv2d(1, 64, kernel_size = 1)\n","        self.cv1x1_4 = nn.Conv2d(1, 128, kernel_size = 1)\n","        \n","    def forward(self, x, z1=None, z2=None, z3=None, z4=None):\n","        \n","        if z4 is None:\n","            z4 = self.cv1x1_1(x)\n","            \n","        if z3 is None:\n","            z3 = self.cv1x1_2(x)\n","            z3 = F.interpolate(z3, size=(64,64))\n","            \n","        if z2 is None:\n","            z2 = self.cv1x1_3(x)\n","            z2 = F.interpolate(z2, size=(32,32))\n","            \n","        if z1 is None:\n","            z1 = self.cv1x1_4(x)\n","            z1 = F.interpolate(z1, size=(16,16))\n","        \n","        x1 = self.relu(self.conv1(x))\n","        x1 = torch.cat((x1, z4), dim = 1) # 32x128x128\n","        x2 = self.relu(self.conv2(x1))\n","        x2 = self.maxpool(x2) # 32x64x64\n","        x2 = torch.cat((x2, z3), dim = 1)\n","        x3 = self.relu(self.conv3(x2))\n","        x3 = self.maxpool(x3) # 64x32x32\n","        x3 = torch.cat((x3, z2), dim = 1)\n","        x4 = self.relu(self.conv4(x3))\n","        x4 = self.maxpool(x4) # 128x16x16\n","        x4 = torch.cat((x4, z1), dim = 1)\n","        x5 = self.relu(self.conv5(x4))\n","        x5 = self.maxpool(x5) # 128x8x8\n","        x5 = x5.view(-1, 128*8*8)\n","        logit = self.fc(x5)\n","        out = self.sigmoid(logit)\n","        return out, logit"],"execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 4, kernel_size = 7, stride =2)#, padding = 1)\n","        self.conv2 = nn.Conv2d(4, 8, kernel_size = 5, stride = 2)#, padding = 1)\n","        self.conv3 = nn.Conv2d(8, 16, kernel_size = 3, stride = 1, padding = 1)\n","        self.fc1 = nn.Linear(16*29*29, 1)\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.sigmoid = nn.Sigmoid()\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        #x = self.maxpool(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        #x = self.maxpool(x)\n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        #x = self.maxpool(x)\n","        x = x.view(-1, 29*29*16)\n","        logit = self.fc1(x)\n","        #logit = self.fc2(x)\n","        x = self.sigmoid(logit)\n","        return x, logit"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# custom weights initialization called on netG and netD\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        '''\n","        Conv2d: w' = (w - k +2p)/s + 1\n","        ConvTr2d: w = s(w'-1) + k -2p\n","        '''\n","        #[b, 512, 4, 4]\n","        self.tr1 = nn.ConvTranspose2d(512, 384, kernel_size = 4, stride = 2, padding = 1) # [b, 384, 8, 8]\n","        self.tr2 = nn.ConvTranspose2d(384, 256, kernel_size = 3, stride = 1, padding = 1) # [b, 256, 8, 8]\n","        self.tr3 = nn.ConvTranspose2d(256, 192, kernel_size = 4, padding = 1, stride = 2) # [b, 192, 16, 16]\n","        self.tr4 = nn.ConvTranspose2d(192, 128, kernel_size = 3, stride = 1, padding = 1) # [b, 128, 16, 16]\n","        self.tr5 = nn.ConvTranspose2d(128, 96, kernel_size = 4, padding = 1, stride = 2) # [b, 96, 32, 32]\n","        self.tr6 = nn.ConvTranspose2d(96, 64, kernel_size = 3, stride = 1, padding = 1) # [b, 64, 32, 32]\n","        self.tr7 = nn.ConvTranspose2d(64, 48, kernel_size = 4, padding = 1, stride = 2) # [b, 48, 32, 32]\n","        self.tr8 = nn.ConvTranspose2d(48, 32, kernel_size = 3, stride = 1, padding = 1) # [b, 32, 64, 64]\n","        self.tr9 = nn.ConvTranspose2d(32, 16, kernel_size = 4, padding = 1, stride = 2) # [b, 16, 64, 64]\n","        self.tr10 = nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 1, padding = 1) # [b, 8, 128, 128]\n","        self.tr11 = nn.ConvTranspose2d(8, 4, kernel_size = 3, padding = 1, stride = 1) # [b, 4, 128, 128]\n","        self.tr12 = nn.ConvTranspose2d(4, 1, kernel_size = 3, stride = 1, padding = 1) # [b, 1, 128, 128]\n","        \n","        \n","        self.bn1 = nn.BatchNorm2d(384)\n","        self.bn2 = nn.BatchNorm2d(256)\n","        self.bn3 = nn.BatchNorm2d(192)\n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.bn5 = nn.BatchNorm2d(96)\n","        self.bn6 = nn.BatchNorm2d(64)\n","        self.bn7 = nn.BatchNorm2d(48)\n","        self.bn8 = nn.BatchNorm2d(32)\n","        self.bn9 = nn.BatchNorm2d(16)\n","        self.bn10 = nn.BatchNorm2d(8)\n","        self.bn11 = nn.BatchNorm2d(4)\n","        \n","        '''\n","        self.tr1 = nn.ConvTranspose2d(256, 128, kernel_size = 4, stride = 2, padding = 1)\n","        self.tr2 = nn.ConvTranspose2d(128, 64, kernel_size = 4, stride = 2, padding = 1)\n","        self.tr3 = nn.ConvTranspose2d(64, 32, kernel_size = 4, stride = 2, padding = 1)\n","        self.tr4 = nn.ConvTranspose2d(32, 16, kernel_size = 3, stride = 1, padding = 1)\n","        self.tr5 = nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 1, padding = 1)\n","        self.tr6 = nn.ConvTranspose2d(8, 1, kernel_size = 3, stride = 1, padding = 1)'''\n","        self.relu = nn.ReLU()\n","        self.fc = nn.Linear(1024, 512*4*4)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, z):\n","        z = self.fc(z)\n","        z = z.view(-1, 512, 4, 4)\n","        z1 = self.bn1(self.relu(self.tr1(z)))\n","        # z = [b, 512, 4, 4], z1 = [b, 384, 8, 8], z2 [b, 256, 8, 8] -> [b, 640, 8, 8]\n","        z2 = self.bn2(self.relu(self.tr2(z1)))\n","        z3 = self.bn3(self.relu(self.tr3(z2)))\n","        z4 = self.bn4(self.relu(self.tr4(z3)))\n","        z5 = self.bn5(self.relu(self.tr5(z4)))\n","        z6 = self.bn6(self.relu(self.tr6(z5)))\n","        z7 = self.bn7(self.relu(self.tr7(z6)))\n","        z8 = self.bn8(self.relu(self.tr8(z7)))\n","        z9 = self.bn9(self.relu(self.tr9(z8)))\n","        z10 = self.bn10(self.relu(self.tr10(z9)))\n","        z11 = self.bn11(self.relu(self.tr11(z10)))\n","        z12 = self.sigmoid(self.tr12(z11))\n","        return z12\n"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Discriminator_2(nn.Module):\n","    '''\n","    Conv2d: w' = (w - k +2p)/s + 1\n","    ConvTr2d: w = s(w'-1) + k -2p\n","    '''    \n","    def __init__(self, in_size = 1, ndf = 64):\n","        super().__init__()       \n","        self.in_size = in_size\n","        self.ndf = ndf\n","        self.main = nn.Sequential(\n","            # input size is in_size x 128 x 128\n","            nn.Conv2d(in_size, self.ndf, kernel_size = 4, stride = 2, padding = 1, bias = False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size: ndf x 64 x 64\n","            nn.Conv2d(self.ndf, self.ndf, kernel_size = 4, stride = 2, padding = 1, bias = False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size: ndf x 32 x 32\n","            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(self.ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size: (ndf * 2) x 16 x 16\n","            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(self.ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size: (ndf * 4) x 8 x 8\n","            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(self.ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size: (ndf * 8) x 4 x 4\n","            nn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","            # state size: 1 x 1 x 1\n","        )\n","        \n","        \n","        \n","    def forward(self, x):\n","        output = self.main(x)\n","        return output"],"execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def reduce_mean(x):\n","    output = torch.mean(x,0, keepdim = False)\n","    output = torch.mean(output,-1, keepdim = False)\n","    return output"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def train_con(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device):\n","    generator.train()\n","    discriminator.train()\n","    lossD = []\n","    lossG = []\n","    sum_D_x = 0\n","    sum_loss_D = 0\n","    sum_loss_G = 0\n","    it = 0\n","    for X in batch_generator(data, batch_size, shuffle=True):\n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        #train with real\n","        X = torch.from_numpy(X).float().to(device)\n","        X = X/127\n","        X = torch.unsqueeze(X,1)\n","        discriminator.zero_grad()\n","        \n","        D, D_logits = discriminator(X)\n","        \n","        \n","\n","        #####loss\n","        d_loss_real = reduce_mean(d_criterion(D_logits, 0.9*torch.ones_like(D)))\n","        d_loss_real.backward()#retain_graph=True)\n","        D_x = D.mean().item()\n","        sum_D_x += D_x \n","        \n","        \n","        #train with fake\n","        \n","        noise = torch.randn(batch_size, 1024, device=device)\n","        \n","\n","        X, z1, z2, z3, z4 = generator(noise)\n","        D_, D_logits_ = discriminator(X, z1, z2, z3, z4)\n","        d_loss_fake = reduce_mean(d_criterion(D_logits_, torch.zeros_like(D_)))\n","        d_loss_fake.backward(retain_graph=True)\n","        \n","        D_G_z1 = D_.mean().item()\n","        \n","        errD = d_loss_fake + d_loss_real\n","        errD = errD.item()\n","        \n","        sum_loss_D += errD\n","        \n","        optimizerD.step()\n","        \n","        \n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        generator.zero_grad()\n","                \n","        D_, D_logits_ = discriminator(X, z1, z2, z3, z4)\n","\n","        ###loss\n","        errG = reduce_mean(g_criterion(D_logits_, torch.ones_like(D_)))\n","        \n","        errG.backward()#retain_graph=True)\n","        D_G_z2 = D_.mean().item()\n","        optimizerG.step()\n","        \n","        sum_loss_G += errG\n","        it += 1\n","        if it % 100 == 0:\n","            print(f'Train | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n","    print(f'Train | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')\n","    \n","\n","\n","def validate_con(discriminator, generator, d_criterion, g_criterion, data, batch_size, epoch, device):\n","    discriminator.eval()\n","    generator.eval()\n","    lossD = []\n","    lossG = []\n","    sum_D_x = 0\n","    sum_loss_D = 0\n","    sum_loss_G = 0\n","    it = 0\n","    with torch.no_grad():\n","        for X in batch_generator(data, batch_size, shuffle=True):\n","            ############################\n","            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","            ###########################\n","            #train with real\n","            X = torch.from_numpy(X).float().to(device)\n","            X = X/127\n","            X = torch.unsqueeze(X,1)\n","\n","            D, D_logits = discriminator(X)\n","\n","\n","\n","            #####loss\n","            d_loss_real = reduce_mean(d_criterion(D_logits, 0.9*torch.ones_like(D)))\n","            D_x = D.mean().item()\n","            sum_D_x += D_x \n","\n","\n","            #train with fake\n","\n","            noise = torch.randn(batch_size, 1024, device=device)\n","\n","\n","            X, z1, z2, z3, z4 = generator(noise)\n","            D_, D_logits_ = discriminator(X)\n","            d_loss_fake = reduce_mean(d_criterion(D_logits_, torch.zeros_like(D_)))\n","\n","            D_G_z1 = D_.mean().item()\n","\n","            errD = d_loss_fake + d_loss_real\n","            errD = errD.item()\n","\n","            sum_loss_D += errD\n","\n","            ############################\n","            # (2) Update G network: maximize log(D(G(z)))\n","            ###########################\n","            D_, D_logits_ = discriminator(X)\n","\n","            ###loss\n","            errG = reduce_mean(g_criterion(D_logits_, torch.ones_like(D_)))\n","            D_G_z2 = D_.mean().item()\n","            sum_loss_G += errG\n","            it += 1\n","            if it % 100 == 0:\n","                print(f'Validation | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n","        print(f'Validation | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def train(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device):\n","    generator.train()\n","    discriminator.train()\n","    lossD = []\n","    lossG = []\n","    sum_D_x = 0\n","    sum_loss_D = 0\n","    sum_loss_G = 0\n","    it = 0\n","    for X in batch_generator(data, batch_size, shuffle=True):\n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        #train with real\n","        X = torch.from_numpy(X).float().to(device)\n","        X = X/127\n","        X = torch.unsqueeze(X,1)\n","        discriminator.zero_grad()\n","        \n","        D, D_logits = discriminator(X)\n","        \n","        \n","\n","        #####loss\n","        d_loss_real = reduce_mean(d_criterion(D_logits, 0.9*torch.ones_like(D)))\n","        d_loss_real.backward()#retain_graph=True)\n","        D_x = D.mean().item()\n","        sum_D_x += D_x \n","        \n","        \n","        #train with fake\n","        \n","        noise = torch.randn(batch_size, 1024, device=device)\n","        \n","\n","        X = generator(noise)\n","        D_, D_logits_ = discriminator(X)\n","        d_loss_fake = reduce_mean(d_criterion(D_logits_, torch.zeros_like(D_)))\n","        d_loss_fake.backward(retain_graph=True)\n","        \n","        D_G_z1 = D_.mean().item()\n","        \n","        errD = d_loss_fake + d_loss_real\n","        errD = errD.item()\n","        \n","        sum_loss_D += errD\n","        \n","        optimizerD.step()\n","        \n","        \n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        generator.zero_grad()\n","                \n","        D_, D_logits_ = discriminator(X)\n","\n","        ###loss\n","        errG = reduce_mean(g_criterion(D_logits_, torch.ones_like(D_)))\n","        \n","        errG.backward()#retain_graph=True)\n","        D_G_z2 = D_.mean().item()\n","        optimizerG.step()\n","        \n","        sum_loss_G += errG\n","        it += 1\n","        if it % 100 == 0:\n","            print(f'Train | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n","    print(f'Train | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')\n","    \n","\n","\n","def validate(discriminator, generator, d_criterion, g_criterion, data, batch_size, epoch, device):\n","    discriminator.eval()\n","    generator.eval()\n","    lossD = []\n","    lossG = []\n","    sum_D_x = 0\n","    sum_loss_D = 0\n","    sum_loss_G = 0\n","    it = 0\n","    with torch.no_grad():\n","        for X in batch_generator(data, batch_size, shuffle=True):\n","            ############################\n","            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","            ###########################\n","            #train with real\n","            X = torch.from_numpy(X).float().to(device)\n","            X = X/127\n","            X = torch.unsqueeze(X,1)\n","\n","            D, D_logits = discriminator(X)\n","\n","\n","\n","            #####loss\n","            d_loss_real = reduce_mean(d_criterion(D_logits, 0.9*torch.ones_like(D)))\n","            D_x = D.mean().item()\n","            sum_D_x += D_x \n","\n","\n","            #train with fake\n","\n","            noise = torch.randn(batch_size, 1024, device=device)\n","\n","\n","            X = generator(noise)\n","            D_, D_logits_ = discriminator(X)\n","            d_loss_fake = reduce_mean(d_criterion(D_logits_, torch.zeros_like(D_)))\n","\n","            D_G_z1 = D_.mean().item()\n","\n","            errD = d_loss_fake + d_loss_real\n","            errD = errD.item()\n","\n","            sum_loss_D += errD\n","\n","            ############################\n","            # (2) Update G network: maximize log(D(G(z)))\n","            ###########################\n","            D_, D_logits_ = discriminator(X)\n","\n","            ###loss\n","            errG = reduce_mean(g_criterion(D_logits_, torch.ones_like(D_)))\n","            D_G_z2 = D_.mean().item()\n","            sum_loss_G += errG\n","            it += 1\n","            if it % 100 == 0:\n","                print(f'Validation | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n","        print(f'Validation | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')"],"execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def train2(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device):\n","    generator.train()\n","    discriminator.train()\n","    lossD = []\n","    lossG = []\n","    sum_D_x = 0\n","    sum_loss_D = 0\n","    sum_loss_G = 0\n","    it = 0\n","    for X in batch_generator(data, batch_size, shuffle=True):\n","        ############################\n","        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","        ###########################\n","        #train with real\n","        X = torch.from_numpy(X).float().to(device)\n","        X = X/127\n","        X = torch.unsqueeze(X,1)\n","        discriminator.zero_grad()\n","        \n","        D = discriminator(X).view(-1)\n","        \n","        \n","\n","        #####loss\n","        d_loss_real = reduce_mean(d_criterion(D, 0.9*torch.ones_like(D)))\n","        d_loss_real.backward()#retain_graph=True)\n","        D_x = D.mean().item()\n","        sum_D_x += D_x \n","        \n","        \n","        #train with fake\n","        \n","        noise = torch.randn(batch_size, 1024, device=device)\n","        \n","\n","        X = generator(noise)\n","        D_ = discriminator(X).view(-1)\n","        d_loss_fake = reduce_mean(d_criterion(D_, torch.zeros_like(D_)))\n","        d_loss_fake.backward(retain_graph=True)\n","        \n","        D_G_z1 = D_.mean().item()\n","        \n","        errD = d_loss_fake + d_loss_real\n","        errD = errD.item()\n","        \n","        sum_loss_D += errD\n","        \n","        optimizerD.step()\n","        \n","        \n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        generator.zero_grad()\n","                \n","        D_ = discriminator(X).view(-1)\n","\n","        ###loss\n","        errG = reduce_mean(g_criterion(D_, torch.ones_like(D_)))\n","        \n","        errG.backward()#retain_graph=True)\n","        D_G_z2 = D_.mean().item()\n","        optimizerG.step()\n","        \n","        sum_loss_G += errG\n","        it += 1\n","        if it % 100 == 0:\n","            print(f'Train | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n","    print(f'Train | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')\n","    \n","\n","\n","def validate2(discriminator, generator, d_criterion, g_criterion, data, batch_size, epoch, device):\n","    discriminator.eval()\n","    generator.eval()\n","    lossD = []\n","    lossG = []\n","    sum_D_x = 0\n","    sum_loss_D = 0\n","    sum_loss_G = 0\n","    it = 0\n","    with torch.no_grad():\n","        for X in batch_generator(data, batch_size, shuffle=True):\n","            ############################\n","            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","            ###########################\n","            #train with real\n","            X = torch.from_numpy(X).float().to(device)\n","            X = X/127\n","            X = torch.unsqueeze(X,1)\n","\n","            D = discriminator(X).view(-1)\n","\n","\n","\n","            #####loss\n","            d_loss_real = reduce_mean(d_criterion(D, 0.9*torch.ones_like(D)))\n","            D_x = D.mean().item()\n","            sum_D_x += D_x \n","\n","\n","            #train with fake\n","\n","            noise = torch.randn(batch_size, 1024, device=device)\n","\n","\n","            X = generator(noise)\n","            D_ = discriminator(X).view(-1)\n","            d_loss_fake = reduce_mean(d_criterion(D_, torch.zeros_like(D_)))\n","\n","            D_G_z1 = D_.mean().item()\n","\n","            errD = d_loss_fake + d_loss_real\n","            errD = errD.item()\n","\n","            sum_loss_D += errD\n","\n","            ############################\n","            # (2) Update G network: maximize log(D(G(z)))\n","            ###########################\n","            D_ = discriminator(X).view(-1)\n","\n","            ###loss\n","            errG = reduce_mean(g_criterion(D_, torch.ones_like(D_)))\n","            D_G_z2 = D_.mean().item()\n","            sum_loss_G += errG\n","            it += 1\n","            if it % 100 == 0:\n","                print(f'Validation | D loss={errD:.6f} | G loss:{errG:.6f} | D(x):{D_x:.6f} | D(G(z)) = {D_G_z1:.4f} / {D_G_z2:.4f}')\n","        print(f'Validation | epoch={epoch:03d} | D loss={sum_loss_D:.6f} | G loss:{sum_loss_G:.6f}')"],"execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["epochs = 10\n","batch_size = 64"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def batch_generator(data, batch_size=1, shuffle=False):\n","    nsamples = len(data)\n","    if shuffle:\n","        perm = np.random.permutation(nsamples)\n","    else:\n","        perm = range(nsamples)\n","\n","    for i in range(0, nsamples, batch_size):\n","        batch_idx = perm[i:i+batch_size]\n","        yield data[batch_idx]"],"execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["d = Discriminator_con().to(device)\n","g = Generator_Res2().to(device)\n","\n","d.apply(weights_init)\n","g.apply(weights_init)\n","\n","optimizerD = torch.optim.Adam(d.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizerG = torch.optim.Adam(g.parameters(), lr=0.001, betas=(0.5, 0.999))\n","\n","d_criterion = nn.BCEWithLogitsLoss()\n","g_criterion = nn.BCEWithLogitsLoss()\n","\n","for epoch in range(epochs):\n","    train_con(d, g, optimizerD, optimizerG, d_criterion, g_criterion, train_data, batch_size=64, epoch=epoch, device=device)\n","    val_loss = validate_con(d, g, d_criterion, g_criterion, train_data, batch_size=64, epoch=epoch, device=device)\n"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"Train | D loss=0.325127 | G loss:11.051830 | D(x):0.899998 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325106 | G loss:12.452294 | D(x):0.900947 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325092 | G loss:19.635309 | D(x):0.899972 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:25.903656 | D(x):0.899971 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325138 | G loss:26.626637 | D(x):0.896836 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:23.127258 | D(x):0.900148 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:23.755939 | D(x):0.900043 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325084 | G loss:20.532911 | D(x):0.899623 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:20.287163 | D(x):0.900035 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:18.341640 | D(x):0.899955 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:17.187706 | D(x):0.900014 | D(G(z)) = 0.0000 / 0.0000\nTrain | epoch=000 | D loss=369.016984 | G loss:22452.414062\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899907 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899904 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899915 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626131 | G loss:0.105531 | D(x):0.899908 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899903 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105531 | D(x):0.899903 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105531 | D(x):0.899908 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899910 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899909 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105531 | D(x):0.899913 | D(G(z)) = 0.8998 / 0.8998\nValidation | D loss=2.626130 | G loss:0.105532 | D(x):0.899905 | D(G(z)) = 0.8998 / 0.8998\nValidation | epoch=000 | D loss=2964.901163 | G loss:119.143700\nTrain | D loss=0.325083 | G loss:17.348915 | D(x):0.900077 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.328259 | G loss:15.632956 | D(x):0.874466 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325084 | G loss:17.084351 | D(x):0.899542 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325230 | G loss:16.978489 | D(x):0.894783 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325086 | G loss:17.236084 | D(x):0.900777 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:34.097862 | D(x):0.900037 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325083 | G loss:21.154385 | D(x):0.899885 | D(G(z)) = 0.0000 / 0.0000\nTrain | D loss=0.325089 | G loss:19.303961 | D(x):0.900992 | D(G(z)) = 0.0000 / 0.0000\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-19-8c913e9fe4eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_con\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizerD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizerG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_con\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-13-d286cf800fe3>\u001b[0m in \u001b[0;36mtrain_con\u001b[1;34m(discriminator, generator, optimizerD, optimizerG, d_criterion, g_criterion, data, batch_size, epoch, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m###########################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#train with real\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m127\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["torch.save(g.state_dict(), 'Generator_Normal_Discriminator2_10epochs_BCE.pt')"],"execution_count":20,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.7.5-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}